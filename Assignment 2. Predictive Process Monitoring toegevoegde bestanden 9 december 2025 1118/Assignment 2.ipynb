{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "indian-mauritius",
   "metadata": {},
   "source": [
    "# Assignment 2 - Predictive Process Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-oracle",
   "metadata": {},
   "source": [
    "**Due: Friday, 19 December, 2025 at 17:00 CET**\n",
    "\n",
    "In this assignment, you will learn to train a regression model to predict the remaining time of a process. Additionally, you will demonstrate your ability to evaluate the model's performance and discuss the results in a report. The learning objectives of this assignment are to:\n",
    "\n",
    "- Apply data cleaning, data transformation, and feature encoding techniques to preprocess event data.\n",
    "- Use regression models to predict the remaining time of ongoing cases.\n",
    "- Calculate model performance metrics (e.g., MAE, MSE, RMSE, \\(R^2\\), etc.).\n",
    "- Refine the experimental design to compare the performance of different preprocessing and encoding methods.\n",
    "- Reflect on the differences between various methods and their effect on the model performance.\n",
    "\n",
    "\n",
    "## Tasks Overview\n",
    "\n",
    "This assignment includes six tasks:\n",
    "\n",
    "1. **Data Exploration:** Perform data exploration to understand the dataset.\n",
    "2. **Data Preprocessing and Trace Encoding:** Apply data preprocessing and trace encoding (covered during Lectures 5 and 7).\n",
    "3. **Regression Model Training:** Select a regression algorithm of interest and train a regression model (a regressor) to forecast the remaining time of each case after each event (see Lecture 6)\n",
    "4. **Prefix-Length Buckets:** Create buckets of different prefix lengths and train a separate regressor for each bucket (covered during Lectures 5 and 7).\n",
    "5. **Alternative Methods:** Revisit your design decisions and investigate two additional methods that may improve model performance.\n",
    "6. **Evaluation:** Evaluate the results.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- You may reuse your code from Tasks 2 and 3 in Assignment 1 for Tasks 4 and 5.\n",
    "- For Task 6 and your report, ensure that you save all the calculated metrics (MAE, MSE, RMSE, and \\(R^2\\)) in previous tasks. Save these metrics in a list or dictionary to facilitate easy evaluation and comparison of results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-pixel",
   "metadata": {},
   "source": [
    "## Task 1: Exploring the data set\n",
    "\n",
    "\n",
    "\n",
    "### Data set: Sepsis\n",
    "\n",
    "Import the file *Complete Sepsis.csv* to load the Sepsis data set. This real-life event log contains events of sepsis cases from a hospital. Sepsis is a life threatening condition typically caused by an infection. One case represents a patient's pathway through the treatment process. The events were recorded by the ERP (Enterprise Resource Planning) system of the hospital. The original data set contains about 1000 cases with in total 15,000 events that were recorded for 16 different activities. Moreover, 39 data attributes are recorded, e.g., the group responsible for the activity, the results of tests and information from checklists. \n",
    "\n",
    "Additional information about the data can be found :\n",
    "- https://data.4tu.nl/articles/dataset/Sepsis_Cases_-_Event_Log/12707639\n",
    "- http://ceur-ws.org/Vol-1859/bpmds-08-paper.pdf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aeb963",
   "metadata": {},
   "source": [
    "### Information about the dataset\n",
    "\n",
    "The dataset used in this assignment is an event log, which records the execution of multiple process instances (cases). In this context, each case represents a patient suspected of having sepsis. The log captures the sequence of clinical activities performed for each patient, where each activity corresponds to a specific diagnostic or treatment-related test. The associated timestamp indicates when the activity was executed.\n",
    "\n",
    "The data mining phase has already been completed. The subsequent objective is to develop predictive models capable of estimating the remaining time of an ongoing case. Specifically, given a newly admitted patient and the set of clinical events recorded so far, the goal is to predict the expected duration of the patient’s hospitalization based on the progression of diagnostic tests and treatments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f65563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9e0cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data   \n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', False)\n",
    "data_Sepsis = pd.read_csv(\"./Complete_Sepsis_Filtered.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-michigan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an overview of the data\n",
    "rows, columns = data_Sepsis.shape[0], data_Sepsis.shape[1]\n",
    "print(f\"Amount of rows: {rows}\\nAmount of features: {columns}\")\n",
    "# print(data_Sepsis.shape)\n",
    "\n",
    "# all column names\n",
    "columns = [col for col in data_Sepsis.columns]\n",
    "print(f\"\\nList of columns: {columns}\")\n",
    "\n",
    "# all unique activities\n",
    "uniq_activities = [activity for activity in data_Sepsis[\"Activity\"].unique()]\n",
    "print(f\"List of all unique activities in col Activity: {uniq_activities}\")\n",
    "print(f\"Amount of unique activities: {len(uniq_activities)}\")\n",
    "\n",
    "# all unique cases (Patients) in hospital\n",
    "uniq_patients = [patient for patient in data_Sepsis['Case ID'].unique()]\n",
    "print(f\"\\nTotal unique patients: {len(uniq_patients)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b0964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datatypes of variables\n",
    "datatypes = data_Sepsis.dtypes\n",
    "\n",
    "# only three features contain purely floats, while some rows contain NaN as well\n",
    "float_cols = data_Sepsis.select_dtypes(include=[\"float64\"]).columns\n",
    "print(f\"These variables are of the datatype float: {list(float_cols)}\")\n",
    "\n",
    "# all labeled as object, while lifecycle:transition: str, 'org:group': str, 'Diagnose': str\n",
    "object_cols = data_Sepsis.select_dtypes(include=['object']).columns\n",
    "print(f\"These variables are of the datatype object: {list(object_cols[3:])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19ab49f",
   "metadata": {},
   "source": [
    "### Features that only contain: True, False or NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb09cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rest is semantically Boolean, this means that they are not Boolean yet. \n",
    "bool_like_cols = [\n",
    "    'InfectionSuspected', 'DiagnosticBlood', 'DisfuncOrg',\n",
    "    'SIRSCritTachypnea', 'Hypotensie', 'SIRSCritHeartRate', 'Infusion',\n",
    "    'DiagnosticArtAstrup', 'DiagnosticIC', 'DiagnosticSputum',\n",
    "    'DiagnosticLiquor', 'DiagnosticOther', 'SIRSCriteria2OrMore',\n",
    "    'DiagnosticXthorax', 'SIRSCritTemperature',\n",
    "    'DiagnosticUrinaryCulture', 'SIRSCritLeucos', 'Oligurie',\n",
    "    'DiagnosticLacticAcid', 'Hypoxie', 'DiagnosticUrinarySediment',\n",
    "    'DiagnosticECG'\n",
    "]\n",
    "\n",
    "for col in bool_like_cols:\n",
    "    print(col, data_Sepsis[col].dropna().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bd0006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean, max, min activities per case\n",
    "events_per_case = data_Sepsis.groupby(\"Case ID\").size()\n",
    "\n",
    "print(events_per_case.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d692bb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency of each age, seeing upwards trend\n",
    "age_counts_df = data_Sepsis[\"Age\"].value_counts(dropna=True).reset_index()\n",
    "age_counts_df.columns = [\"Age\", \"Count\"]\n",
    "\n",
    "# for loop calculating percentage of ages   \n",
    "total = 0\n",
    "percentage = []\n",
    "for count in age_counts_df[\"Count\"]:\n",
    "    total += count\n",
    "    percentage.append(count/777 * 100)\n",
    "    \n",
    "age_counts_df['Percentage'] = percentage\n",
    "\n",
    "i = 0\n",
    "for percentage in age_counts_df['Percentage'][0:6]: \n",
    "    i += percentage\n",
    "    \n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587c5a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot\n",
    "ax.bar(age_counts_df[\"Age\"], age_counts_df[\"Count\"], width=3)\n",
    "\n",
    "# Axis labels\n",
    "ax.set_xlabel(\"Age\")\n",
    "ax.set_ylabel(\"Number of Patients\")\n",
    "ax.set_title(\"Age Distribution of Patients\")\n",
    "\n",
    "# Age axis ticks every 5 years\n",
    "ax.set_xticks(range(int(age_counts_df[\"Age\"].min()),\n",
    "                   int(age_counts_df[\"Age\"].max()) + 1, 5))\n",
    "\n",
    "# Frequency axis ticks every 10\n",
    "ax.set_yticks(range(0, age_counts_df[\"Count\"].max() + 10, 10))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ddf7d3-ab12-4bc1-b052-5fa539115f0a",
   "metadata": {},
   "source": [
    "### Create the Labels by Calculating the Remaining Time\n",
    "\n",
    "To forecast the remaining time for each patient in the hospital, we group the events by patient, use the completion time of each patient (i.e., the timestamp of the last event for each patient), and calculate the time difference between the current event and the completion time. This is done for each event. As a result, we now have our labels, which indicate how long a patient will remain in the treatment process for each event.\n",
    "\n",
    "To help you get started, we created the target variable *remaining_time(days)* for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445dabbf-00b3-4ebb-9bd1-31b6e8484bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_Sepsis.copy()\n",
    "\n",
    "# Convert the timestamp column to datetime\n",
    "df['Complete Timestamp'] = pd.to_datetime(df['Complete Timestamp'])\n",
    "\n",
    "# Find the completion time for each case\n",
    "completion_times = df.groupby('Case ID')['Complete Timestamp'].max().rename('completion_time')\n",
    "\n",
    "# Merge completion time back into the original DataFrame\n",
    "df = df.merge(completion_times, on='Case ID')\n",
    "\n",
    "# Calculate the remaining time for each event\n",
    "df['remaining_time'] = df['completion_time'] - df['Complete Timestamp']\n",
    "\n",
    "# Calculate the remaining time for each event in days\n",
    "df['remaining_time(days)'] = df['remaining_time'].dt.total_seconds()/60/60/24\n",
    "\n",
    "# Retain rows where the remaining time is larger than 0 days. \n",
    "df = df[df['remaining_time(days)'] > 0]\n",
    "\n",
    "# Drop the completion_time column to avoid information leakage\n",
    "df = df.drop(columns=['completion_time'])\n",
    "\n",
    "# Drop the remaining_time column to avoid information leakage\n",
    "df = df.drop(columns=['remaining_time'])\n",
    "\n",
    "# Display the result\n",
    "label_column = 'remaining_time(days)'\n",
    "# print(df['remaining_time(days)'].describe())\n",
    "\n",
    "# Fill in the column names of case id, activity, and time stamps\n",
    "column_Sepsis_CaseID = 'Case ID' \n",
    "column_Sepsis_Activity = 'Activity'\n",
    "column_Sepsis_Timestamps = 'Complete Timestamp'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-queen",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1 Exploratory data analysis\n",
    "\n",
    "For the data set, create 2-3 figures/tables that help you understand the data \n",
    "\n",
    "Note that some of these variables are categorical variables and some are numberical. Additionally, some of the variables have missing values. Think/discuss how would you preprocess these variables.\n",
    "\n",
    "\n",
    "Make sure to at least check each variable's data type and understand their distribution. \n",
    "\n",
    "*For creating data visualizations, you may consider using the matplot library and visit the [matplot gallery](https://matplotlib.org/stable/gallery/index.html) for inspiration (e.g., histograms for distribution, or heatmaps for feature correlation).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef33e54",
   "metadata": {},
   "source": [
    "\n",
    "### Markdown added by me (Mart)\n",
    "Note that some of these variables are categorical variables and some are numberical. Additionally, some of the variables have missing values. Think/discuss how would you preprocess these variables.\n",
    "\n",
    "Many attributes in the Sepsis log contain NaN values. In this context, NaN does not indicate a missing measurement error but the fact that a diagnostic test was not performed at that point in the process. This absence is clinically meaningful and must therefore be preserved. Consequently, these attributes will be treated as categorical three-level variables (True / False / Not performed), rather than being imputed or discarded.\n",
    "\n",
    "Make sure to at least check each variable's data type and understand their distribution - All the variables data types have been outputted above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1f5eed",
   "metadata": {},
   "source": [
    "### Figure 1 — Distribution of Remaining Time Across Events\n",
    "\n",
    "This plot shows how many events occur at different remaining times (measured in days). Most events happen when the remaining time is still very short (0–10 days), and the frequency quickly drops as the remaining time increases. The long tail on the right indicates that a few patients stay in the process much longer than the rest.\n",
    "\n",
    "This skewed distribution is important because it tells us that predicting long remaining times will be harder for the model, simply because these cases are much less common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-tournament",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of remaining time across events. \n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(df['remaining_time(days)'], bins=40)\n",
    "plt.xlabel(\"Remaining Time (days)\")\n",
    "plt.ylabel(\"Number of Events\")\n",
    "plt.title(\"Distribution of Remaining Time Across Events\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525bac96",
   "metadata": {},
   "source": [
    "### Figure 2 — Frequency of Activities in the Sepsis Log\n",
    "\n",
    "This bar chart shows how often each activity appears in the event log. A few activities (like Leucocytes, CRP, and LacticAcid measurements) occur very frequently, while others are much rarer. This means that most patient traces follow similar diagnostic steps, and a small number of activities only appear in specific situations.\n",
    "\n",
    "Understanding which activities dominate the process helps us later when we encode traces, since common activities will have a much bigger influence on the model than rare ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fffe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "df['Activity'].value_counts().plot(kind='bar')\n",
    "plt.xlabel(\"Activity\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Frequency of Activities in the Sepsis Log\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019e31de",
   "metadata": {},
   "source": [
    "### Figure 3 — Distributions of Numerical Clinical Attributes\n",
    "\n",
    "These histograms show the distributions of the main numerical clinical measurements: Leucocytes, CRP, and LacticAcid. All three are strongly right-skewed, meaning most values are relatively low, with some patients showing very high values.\n",
    "\n",
    "This pattern is typical in medical data and suggests that:\n",
    "- we may want to apply scaling or transformation later, and  \n",
    "- the extreme values likely reflect real clinical severity rather than noise.\n",
    "\n",
    "These plots give us a first idea of how these variables behave and how they might contribute to predicting remaining time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b75c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Leucocytes', 'CRP', 'LacticAcid']\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "for i, col in enumerate(cols, 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    df[col].hist(bins=30)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-imperial",
   "metadata": {},
   "source": [
    "### (Optional) 1.2 Process Discovery and Visualization \n",
    "\n",
    "This is an optional task to show you how process discovery and visualizaion can be deployed using the pm4py library. \n",
    "\n",
    "(*The following code requires the graphviz library to be installed. If you have issues with installing the graphviz, you may try to follow the instructions on Install GraphViz on the [pm4py](https://pm4py.fit.fraunhofer.de/install-page) install page*)\n",
    "\n",
    "The following code:\n",
    "- fill in the columns for case id, activity, and timestamps\n",
    "- convert the data set into an event log\n",
    "- discover a Directly-follows graph (DFG) and a process model for each event log. \n",
    "- you may use the discovered process model in your report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-still",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have brew installed, install graphviz brew install graphviz\n",
    "# Then install pm4py library and graphviz library using pip install \n",
    "# !pip install -U pm4py\n",
    "# !conda install graphviz\n",
    "# !conda install python-graphviz\n",
    "import pm4py\n",
    "\n",
    "\n",
    "data_Sepsis[column_Sepsis_CaseID] = data_Sepsis[column_Sepsis_CaseID].astype(str)\n",
    "\n",
    "data_Sepsis_copy = data_Sepsis.copy()\n",
    "data_Sepsis_copy['Complete Timestamp'] = pd.to_datetime(data_Sepsis_copy['Complete Timestamp'])\n",
    "\n",
    "# Convert the data to an event log\n",
    "log_Sepsis = pm4py.format_dataframe(data_Sepsis_copy, case_id=column_Sepsis_CaseID, activity_key=column_Sepsis_Activity, timestamp_key=column_Sepsis_Timestamps)\n",
    "\n",
    "# Set the log to be the one that you are interested\n",
    "log = log_Sepsis\n",
    "\n",
    "# Create a Directly-Follows Graph (DFG) and plot this graph\n",
    "dfg, start_activities, end_activities = pm4py.discover_dfg(log)\n",
    "pm4py.view_dfg(dfg, start_activities, end_activities)\n",
    "\n",
    "# Discover a Process Model using Inductive Miner and plot this BPMN model\n",
    "process_tree = pm4py.discover_process_tree_inductive(log)\n",
    "bpmn_model = pm4py.convert_to_bpmn(process_tree)\n",
    "pm4py.view_bpmn(bpmn_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-distinction",
   "metadata": {},
   "source": [
    "## Task 2: Preprocessing and Trace Encoding\n",
    "\n",
    "### 2.1 Data preprocessing\n",
    "\n",
    "In the previous data exploration task, you gathered some initial insights about the dataset. Based on your observations during data exploration, decide which preprocessing steps are necessary (e.g., handling missing values, encoding categorical variables, scaling numerical features, etc.) and implement them accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9734f46b",
   "metadata": {},
   "source": [
    "### Handling Missing Values\n",
    "\n",
    "#### Boolean Clinical Test Columns (True / False / NaN)\n",
    "\n",
    "During exploration, we observed that many clinical indicators (e.g., `InfectionSuspected`, `DiagnosticBlood`, `SIRSCritTemperature`, etc.) contain the values `True`, `False`, and `NaN`. Importantly, the NaN values do **not** represent missing measurements in the traditional sense. Instead, they indicate that **a specific diagnostic test was not performed at that event**. This distinction is clinically meaningful.\n",
    "\n",
    "For example, a patient might not undergo a lactic acid test early in their trajectory, and the absence of this test is informative: it often correlates with clinical stability. Because of this, we cannot impute NaN values with True/False, nor can we drop the rows.\n",
    "\n",
    "**Decision:**  \n",
    "Treat these variables as **categorical features with three states**:\n",
    "\n",
    "- `0` = False (test performed and negative/normal)  \n",
    "- `1` = True  (test performed and positive/abnormal)  \n",
    "- `2` = Not performed (originally NaN)\n",
    "\n",
    "This encoding preserves the semantic meaning of the missing values and makes the features usable for downstream modeling.\n",
    "\n",
    "**Justification from EDA:**  \n",
    "Plots and unique-value summaries clearly showed that NaN values dominate several clinical test columns. Treating them as an informative category rather than noise ensures that the model can learn the difference between “test not performed” and “test performed but normal.”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709405c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac367fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: preprocessing the boolean like columns\n",
    "bool_like_cols = [\n",
    "    'InfectionSuspected', 'DiagnosticBlood', 'DisfuncOrg', 'SIRSCritTachypnea',\n",
    "    'Hypotensie', 'SIRSCritHeartRate', 'Infusion', 'DiagnosticArtAstrup',\n",
    "    'DiagnosticIC', 'DiagnosticSputum', 'DiagnosticLiquor', 'DiagnosticOther',\n",
    "    'SIRSCriteria2OrMore', 'DiagnosticXthorax', 'SIRSCritTemperature',\n",
    "    'DiagnosticUrinaryCulture', 'SIRSCritLeucos', 'Oligurie',\n",
    "    'DiagnosticLacticAcid', 'Hypoxie', 'DiagnosticUrinarySediment',\n",
    "    'DiagnosticECG'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e50a1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns above are replaced by integers: True = 1, False = 0 and NaN = 2\n",
    "df_pre[bool_like_cols] = (\n",
    "    df_pre[bool_like_cols]\n",
    "    .fillna(2)\n",
    "    .replace({False: 0, True: 1})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564a5dfa",
   "metadata": {},
   "source": [
    "#### Numerical Clinical Measurements (Age, Leucocytes, CRP, LacticAcid)\n",
    "\n",
    "These four variables were the only continuous numerical features in the dataset. The visualizations in Task 1 showed that all of them are **heavily right-skewed with extreme outliers**, which is expected in medical data.\n",
    "\n",
    "Missing values in these columns represent the **absence of a recorded measurement**, not a meaningful category. For instance, not every patient has a CRP or lactate value associated with every event.\n",
    "\n",
    "**Decision:**  \n",
    "Impute missing values using the **median** of each column.\n",
    "\n",
    "**Why median instead of mean?**\n",
    "\n",
    "- Median is robust to outliers, which are very common in sepsis (e.g., extremely high CRP or lactate levels).\n",
    "- Imputing with the mean would artificially inflate the values due to skewness.\n",
    "- Median imputation preserves the general distribution without distorting the feature space.\n",
    "\n",
    "**Justification from EDA:**  \n",
    "Histograms of Leucocytes, CRP, and Lactic Acid revealed long right tails. Median imputation is therefore the most stable and statistically appropriate choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286abf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['Leucocytes', 'CRP', 'LacticAcid']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    df_pre[col] = df_pre[col].fillna(df_pre[col].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc308b21",
   "metadata": {},
   "source": [
    "#### Dropping columns\n",
    "\n",
    "During exploration, we found that the `Diagnose` column contains categorical codes such as A/B/C and also NaN values. However:\n",
    "\n",
    "- This diagnosis is assigned **after the process has completed**.\n",
    "- It would not be known at prediction time.\n",
    "- Including it would introduce **label leakage** and artificially improved model performance.\n",
    "\n",
    "The column `lifecycle:transition` contains the element: `complete`. This feature carries zero variance and adds no predictive value.\n",
    "\n",
    "**Decision:**  \n",
    "Drop `Diagnose` and `lifecycle:transition` columns entirely.\n",
    "\n",
    "**Justification:**  \n",
    "The column lifecycle:transition was removed because it does not contribute any meaningful information to the prediction task. The lifecycle:transition attribute contains only a single value (“complete”) for all events, which means it provides no variance and cannot support learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f03e470",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre = df_pre.drop(columns=[\"Diagnose\", \"lifecycle:transition\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ab0ebc",
   "metadata": {},
   "source": [
    "#### Encoding `org:group`\n",
    "\n",
    "The attribute `org:group` indicates which organizational unit (e.g., department) is responsible for executing an event. Since it is a low-cardinality categorical feature, it was one-hot encoded using `pd.get_dummies`. This creates a separate binary feature for each group (e.g., `org_A`, `org_B`, …), allowing the model to capture differences between units without imposing an artificial numeric ordering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314c3c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre = pd.get_dummies(df_pre, columns=['org:group'], prefix='org')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d9cf24",
   "metadata": {},
   "source": [
    "### Scaling Numerical Features\n",
    "\n",
    "The numeric attributes `Age`, `Leucocytes`, `CRP`, and `LacticAcid` were scaled using the MinMaxScaler. This transformation maps each value to the range [0, 1] based on the minimum and maximum values of the column.\n",
    "\n",
    "We chose MinMax scaling because the numerical variables in this dataset are highly skewed and contain significant outliers, as observed during exploratory analysis. Standardization (using mean and standard deviation) assumes a roughly normal distribution, which does not hold here. MinMaxScaler preserves the shape of the distribution while ensuring that all numeric features contribute proportionally during model training.\n",
    "\n",
    "Scaling is essential to prevent differences in feature magnitude from influencing the regression model, especially when combining clinical values that span very different numeric ranges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5466ebc4",
   "metadata": {},
   "source": [
    "Finally, the column `remaining_time(days)` is retained as the regression target. It is not scaled or encoded at this stage, since it represents the label that the model will learn to predict in later tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56489187",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['Age', 'Leucocytes', 'CRP', 'LacticAcid']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "df_pre[numeric_cols] = scaler.fit_transform(df_pre[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53e1725-dd60-4a79-aefa-5d83e7af69cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enumerate Activity column\n",
    "activities = {}\n",
    "i = 0\n",
    "for activity in df_pre['Activity'].unique():\n",
    "    activities[activity] = i\n",
    "    i += 1\n",
    "\n",
    "df_pre['Activity'] = df_pre['Activity'].map(activities)\n",
    "\n",
    "# Isolate datetime features\n",
    "df_pre['Hour'] = df_pre['Complete Timestamp'].dt.hour\n",
    "df_pre['Weekday'] = df_pre['Complete Timestamp'].dt.weekday\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87db2911-a575-46a2-9210-e93bf988d9cd",
   "metadata": {},
   "source": [
    "### 2.2 Trace Encoding\n",
    "\n",
    "\n",
    "- Implement the aggregation encoding for the data set (for example, see [1], Table 6)\n",
    "\n",
    "<span style=\"color:gray\">[1] Ilya Verenich, Marlon Dumas, Marcello La Rosa, Fabrizio Maria Maggi, Irene Teinemaa:\n",
    "Survey and Cross-benchmark Comparison of Remaining Time Prediction Methods in Business Process Monitoring. ACM Trans. Intell. Syst. Technol. 10(4): 34:1-34:34 (2019) [Section 1, 2, 4.1, 4.3, 4.6, 5.2, 5.3, 5.4, and 6] </span>\n",
    "\n",
    "This encoding has been discussed during lecture 7.\n",
    "- for the aggregation encoding check the pandas groupby.DataFrameGroupBy and cumsum function and read the [examples and answers on the stake overflow](https://stackoverflow.com/a/49578219)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-street",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# TODO: Implement the function that returns the aggregation state encoding of a log\n",
    "def agg_per_event_encoding(dataFrame, columnCase, columnActivity):\n",
    "    _new_data = dataFrame.copy()\n",
    "    \n",
    "    # TODO: Apply one-hot encoding to the 'activity' column\n",
    "    onehot = pd.get_dummies(_new_data[columnActivity], prefix=columnActivity)\n",
    "    \n",
    "    # TODO: Group by 'case id' and compute the cumulative sum for each activity\n",
    "    cumulative_freq = onehot.groupby(_new_data[columnCase]).cumsum()\n",
    "    \n",
    "    # TODO: Concatenate the original DataFrame with the cumulative frequencies\n",
    "    _data_with_features = pd.concat([_new_data, cumulative_freq], axis=1)\n",
    "    \n",
    "    return(_data_with_features)\n",
    "\n",
    "\n",
    "\n",
    "# For the data set, apply the aggregated state encoding\n",
    "data_Sepsis_ag = agg_per_event_encoding(df_pre, column_Sepsis_CaseID, column_Sepsis_Activity)\n",
    "print(data_Sepsis_ag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-complaint",
   "metadata": {},
   "source": [
    "### 2.3 Create training and test data sets using temporal split\n",
    "\n",
    "\n",
    "Choose the size of your test data and use that to find the appropiate date (time threshold) to split the dataset into training  and test set.\n",
    "\n",
    "This approach is commonly used for time-series or event log data to ensure that training data comes from earlier time periods and test data from later periods. This avoids data leakage, where future data might influence the training process.\n",
    "\n",
    "When writing your report, explain how you split the data and provide a justification for your choice as part of the experiment setup discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f4b5b0-9997-4c41-9567-46829680f040",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define time threshold\n",
    "time_threshold = pd.Timestamp('2014-09-30 00:00:00')\n",
    "\n",
    "# Split the case \n",
    "def train_test_time_based_split(data_frame, time_threshold, column_case_id, column_activity, column_time_stamp, column_label):\n",
    "    # Identify the start time of each case\n",
    "    case_start_dates = data_frame.groupby(column_case_id)[column_time_stamp].min() # don't know if the .min() is really needed, but just in case\n",
    "    \n",
    "    # Separate case IDs into training and test sets\n",
    "    train_case_ids = case_start_dates[case_start_dates < time_threshold].index\n",
    "    test_case_ids = case_start_dates[case_start_dates >= time_threshold].index\n",
    "\n",
    "    # Assign rows to training and test sets based on case IDs\n",
    "    train_set = data_frame[data_frame[column_case_id].isin(train_case_ids)]\n",
    "    test_set = data_frame[data_frame[column_case_id].isin(test_case_ids)]\n",
    "\n",
    "    # Create the training and test sets, while dropping the irrelevant columns \n",
    "    X_train = train_set.drop(columns=[column_label])\n",
    "    y_train = train_set[column_label]\n",
    "    \n",
    "    X_test = test_set.drop(columns=[column_label])\n",
    "    y_test = test_set[column_label]\n",
    "    return (X_train, X_test, y_train, y_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_time_based_split(\n",
    "    data_Sepsis_ag, time_threshold, column_Sepsis_CaseID, \n",
    "    column_Sepsis_Activity, column_Sepsis_Timestamps, label_column)\n",
    "\n",
    "# Confirm the split by checking sizes and that time ranges do not overlap\n",
    "print(\"x train length:\", len(X_train))\n",
    "print(\"x test length:\", len(X_test))\n",
    "print(\"No row overlap:\",  set(X_train[column_Sepsis_CaseID]).isdisjoint(set(X_test[column_Sepsis_CaseID]))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bd8aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results objects.\n",
    "# Gebruik deze objecten om de resultaten van de verschillende modellen in op te slaan,\n",
    "# Dan worden ze later gebruikt in de tabel bij opracht 6\n",
    "\n",
    "no_bucket_agg_train_dt = {\n",
    "    \"mae\": 0.0,\n",
    "    \"mse\": 0.0,\n",
    "    \"rmse\": 0.0,\n",
    "    \"r2\": 0.0\n",
    "}\n",
    "\n",
    "no_bucket_agg_test_dt = {\n",
    "    \"mae\": 0.0,\n",
    "    \"mse\": 0.0,\n",
    "    \"rmse\": 0.0,\n",
    "    \"r2\": 0.0\n",
    "}\n",
    "\n",
    "prefix_5_train_dt = {\n",
    "    \"mae\": 0.0,\n",
    "    \"mse\": 0.0,\n",
    "    \"rmse\": 0.0,\n",
    "    \"r2\": 0.0\n",
    "}\n",
    "\n",
    "prefix_5_test_dt = {\n",
    "    \"mae\": 0.0,\n",
    "    \"mse\": 0.0,\n",
    "    \"rmse\": 0.0,\n",
    "    \"r2\": 0.0\n",
    "}\n",
    "\n",
    "prefix_10_train_dt = {\n",
    "    \"mae\": 0.0,\n",
    "    \"mse\": 0.0,\n",
    "    \"rmse\": 0.0,\n",
    "    \"r2\": 0.0\n",
    "}\n",
    "\n",
    "prefix_10_test_dt = {\n",
    "    \"mae\": 0.0,\n",
    "    \"mse\": 0.0,\n",
    "    \"rmse\": 0.0,\n",
    "    \"r2\": 0.0\n",
    "}\n",
    "\n",
    "prefix_15_train_dt = {\n",
    "    \"mae\": 0.0,\n",
    "    \"mse\": 0.0,\n",
    "    \"rmse\": 0.0,\n",
    "    \"r2\": 0.0\n",
    "} \n",
    "\n",
    "prefix_15_test_dt = {\n",
    "    \"mae\": 0.0,\n",
    "    \"mse\": 0.0,\n",
    "    \"rmse\": 0.0,\n",
    "    \"r2\": 0.0\n",
    "}\n",
    "\n",
    "last_state_train_dt = {\n",
    "    \"mae\": 0.0,\n",
    "    \"mse\": 0.0,\n",
    "    \"rmse\": 0.0,\n",
    "    \"r2\": 0.0\n",
    "}\n",
    "\n",
    "last_state_test_dt = {\n",
    "    \"mae\": 0.0,\n",
    "    \"mse\": 0.0,\n",
    "    \"rmse\": 0.0,\n",
    "    \"r2\": 0.0\n",
    "}\n",
    "\n",
    "last_state_additional_train_dt = {\n",
    "    \"mae\": 0.0,\n",
    "    \"mse\": 0.0,\n",
    "    \"rmse\": 0.0,\n",
    "    \"r2\": 0.0\n",
    "}\n",
    "\n",
    "last_state_additional_test_dt = {\n",
    "    \"mae\": 0.0,\n",
    "    \"mse\": 0.0,\n",
    "    \"rmse\": 0.0,\n",
    "    \"r2\": 0.0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-quantum",
   "metadata": {},
   "source": [
    "## Task 3: Predicting Case Remaining Time \n",
    "\n",
    "\n",
    "In this task, you will train a regression model (aka regressor) to predict case remaining time. \n",
    "You may choose the regression tree, the random forest regression, the kNN-regressor, or the MLP for regression. Very similar to how you have trained a classification model in Assignment 1, now perform the following steps to train a regression model. \n",
    "\n",
    "A) use the default values for the parameters to get a regressor on the training data. \n",
    "- [Regression Tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor)\n",
    "- [Random Forest Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n",
    "\n",
    "(OPTIONAL) use 5-fold cross-validation to tune the parameters. And create plots to show how the selected parameters affect the performance. \n",
    "\n",
    "B) select the best-performing regressor (e.g., the default one or the one that achieved the lowest error) and report the error measures (MAE, MSE, RMSE, R^2) of the fitted model on the test data. \n",
    "\n",
    "    \n",
    "#### TIPS:\n",
    "In case you decide to perform cross-validation, you are allowed to reuse some of your code from Assignment 1 or use the [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) class (see an [example](https://www.dezyre.com/recipes/find-optimal-parameters-using-gridsearchcv-for-regression), but be aware that GridSearchSV does not return MAE or the other error measures (e.g., MSE, RMSE, R^2), you will need to update the scoring function)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-saturn",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def runRegressor(x_train, y_train, x_test, y_test):\n",
    "    x_train.drop(columns=['Case ID', 'Complete Timestamp'], inplace=True, errors='ignore')\n",
    "    x_test.drop(columns=['Case ID', 'Complete Timestamp'], inplace=True, errors='ignore')\n",
    "    regressor = RandomForestRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=6,\n",
    "        min_samples_leaf=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    regressor.fit(x_train, y_train)\n",
    "    \n",
    "    y_pred = regressor.predict(x_test)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    return regressor, mae, mse, rmse, r2\n",
    "\n",
    "_, mae, mse, rmse, r2 = runRegressor(X_train, y_train, X_test, y_test)\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"Mean Squared Error: {mse}\") \n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "print(f\"R^2 Score: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef067c7f-fc9a-4852-a13a-409032ca3d38",
   "metadata": {},
   "source": [
    "## Task 4. Create three buckets and repeat Tasks 2 and 3 for each bucket. \n",
    "\n",
    "In this task, you will create three buckets, for prefix length 5, 10 and 15. For each of the bucket, repeat Task 2 and 3.  \n",
    "\n",
    "You may use the functions you built for Tasks 2 and 3 or reuse code. \n",
    "\n",
    "Calculate the error measures (MAE, MSE, RMSE, R^2) and discuss the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4749491b-1ac3-4800-a2fd-5e012b52274f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: import packages\n",
    "\n",
    "\n",
    "# Group by case_id and create a prefix column\n",
    "df_pre['prefix'] = df_pre.groupby(column_Sepsis_CaseID).cumcount() + 1\n",
    "\n",
    "# Buckets for prefix lengths 5, 10, and 15\n",
    "buckets = {5: [], 10: [], 15:[]}\n",
    "\n",
    "for prefix_length in buckets.keys():\n",
    "    selected_rows = df_pre[df_pre['prefix'] <= prefix_length]\n",
    "    buckets[prefix_length] = selected_rows.copy()\n",
    "\n",
    "results = {}\n",
    "regressors = {}\n",
    "\n",
    "for prefix_length, bucket_df in buckets.items():\n",
    "    if not bucket_df.empty:\n",
    "        \n",
    "        # TODO: Apply the encoding\n",
    "        bucket_df_Sepsis_ag = agg_per_event_encoding(bucket_df, column_Sepsis_CaseID, column_Sepsis_Activity)\n",
    "        \n",
    "        # TODO: Retain the event of prefix_length\n",
    "        bucket_df_Sepsis_ag = bucket_df_Sepsis_ag[bucket_df_Sepsis_ag['prefix'] == prefix_length]\n",
    "       \n",
    "        # TODO: Create train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_time_based_split(\n",
    "            bucket_df_Sepsis_ag, time_threshold, column_Sepsis_CaseID, \n",
    "            column_Sepsis_Activity, column_Sepsis_Timestamps, label_column)\n",
    "\n",
    "        # TODO: Train a regression model (e.g., Random Forest Regressor)\n",
    "        # TODO: Evaluate the regressor by calculating the MAE, etc...\n",
    "        reg, mae, mse, rmse, r2 = runRegressor(X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "        # TODO: Store/print your results\n",
    "        results[prefix_length] = {}\n",
    "        results[prefix_length]['mae'] = mae\n",
    "        results[prefix_length]['mse'] = mse\n",
    "        results[prefix_length]['rmse'] = rmse\n",
    "        results[prefix_length]['r2'] = r2\n",
    "        # Store the trained regressor\n",
    "        regressors[prefix_length] = reg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-apache",
   "metadata": {},
   "source": [
    "## Task 5. Two Alternative Preprocessing and Encoding Methods\n",
    "\n",
    "In this task, you will refine the design of your method so far and compare the performance of different preprocessing and encoding methods. **Choose two of the following options**:\n",
    "\n",
    "A) If you have dropped all features except the *activities* in Task 2, select a few features (e.g., Age, Leukocytes, CRP, Lactic Acid), encode them, and repeat Tasks 2 and 3. Motivate your selection in your report. \n",
    "\n",
    "B) If you already included some features in Task 2, drop all features except the encoded *activities*, and repeat Task 3.\n",
    "\n",
    "C) Engineer a feature called *elapsed time* by computing the time elapsed since the case started until the current event, and repeat Tasks 2 and 3. Evaluate if adding this feature (*elapsed time*) help improve the model performance. \n",
    "\n",
    "D) Use *last-state encoding* instead of aggregation encoding, and repeat Tasks 2 and 3. Evaluate if the *last-state encoding* help improve the model performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-workshop",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adult-equilibrium",
   "metadata": {},
   "source": [
    "## Task 6.  Report your results and discuss your findings\n",
    "\n",
    "By now, you have trained and applied a regressor and evaluated its performance on \n",
    "(1) the non-bucketed training and test set with all possible prefix length. \n",
    "(2) three buckets of different prefix length. \n",
    "(3 & 4) and two other methods you tried (e.g., a different encoding or with additional features). \n",
    "\n",
    "You have created tables or figures which you can add to your report. \n",
    "\n",
    "Create an overview table or figure that compares the performance of each method on the data set, for example, see the table here below. \n",
    "\n",
    "\n",
    "Discuss your findings and reflect on the following questions in your report:\n",
    "- According to the error measures, which one would you suggest as the optimal method (preprocessing + encoding + algorithm)? \n",
    "- Are there any discrepancies between the MAE, MSE, RMSE, and R^2 measures in terms of which model/method performs the best? If yes, how would you explain these discrepancies. \n",
    "- Which one of the MAE, MSE, RMSE, and R^2 would you use for selecting the model? Why?\n",
    "- Which one of the encoding would you suggest for this data set? Why?\n",
    "- Which features have a big influence on predicting the remaining time?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| Encoding | Model | Training MAE  | Test MAE |  Training MSE  |  Test MSE  | Training R^2 | Test R^2 |... |\n",
    "|------|------|------|------|------|------|------|------|-----|\n",
    "|  Agg-state and no bucketing |  RF regressor |  |  | || | |\n",
    "|  Agg-state and prefix length 5 |RF regressor       |  |  | || | |\n",
    "|   Agg-state and prefix length 10 |RF regressor     |  |  | || | |\n",
    "|   Agg-state and prefix length 15 |RF regressor     |  |  | || | |\n",
    "|   last-state |RF regressor    |  |  | || | |\n",
    "|   last-state + additional features |RF regressor    |  |  | || | |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ba6a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "results = []\n",
    "\n",
    "results.append({\n",
    "    'Encoding': 'Agg-state and no bucketing',\n",
    "    'Model': 'RF regressor',\n",
    "    'Training MAE': no_bucket_agg_train_dt[\"mae\"],  \n",
    "    'Test MAE': no_bucket_agg_test_dt[\"mae\"],\n",
    "    'Training MSE': no_bucket_agg_train_dt[\"mse\"],\n",
    "    'Test MSE': no_bucket_agg_test_dt[\"mse\"],\n",
    "    'Training RMSE': no_bucket_agg_train_dt[\"rmse\"],\n",
    "    'Test RMSE': no_bucket_agg_test_dt[\"rmse\"],\n",
    "    'Training R^2': no_bucket_agg_train_dt[\"r2\"],\n",
    "    'Test R^2': no_bucket_agg_test_dt[\"r2\"]\n",
    "})\n",
    "\n",
    "results.append({\n",
    "    'Encoding': 'Agg-state and prefix length 5',\n",
    "    'Model': 'RF regressor',\n",
    "    'Training MAE': prefix_5_train_dt[\"mae\"], \n",
    "    'Test MAE': prefix_5_test_dt[\"mae\"],\n",
    "    'Training MSE': prefix_5_train_dt[\"mse\"],\n",
    "    'Test MSE': prefix_5_test_dt[\"mse\"],\n",
    "    'Training RMSE': prefix_5_train_dt[\"rmse\"],\n",
    "    'Test RMSE': prefix_5_test_dt[\"rmse\"],\n",
    "    'Training R^2': prefix_5_train_dt[\"r2\"],\n",
    "    'Test R^2': prefix_5_test_dt[\"r2\"]\n",
    "})\n",
    "\n",
    "results.append({\n",
    "    'Encoding': 'Agg-state and prefix length 10',\n",
    "    'Model': 'RF regressor',\n",
    "    'Training MAE': prefix_10_train_dt[\"mae\"],\n",
    "    'Test MAE': prefix_10_test_dt[\"mae\"],\n",
    "    'Training MSE': prefix_10_train_dt[\"mse\"],\n",
    "    'Test MSE': prefix_10_test_dt[\"mse\"],\n",
    "    'Training RMSE': prefix_10_train_dt[\"rmse\"],\n",
    "    'Test RMSE': prefix_10_test_dt[\"rmse\"],\n",
    "    'Training R^2': prefix_10_train_dt[\"r2\"],\n",
    "    'Test R^2': prefix_10_test_dt[\"r2\"]\n",
    "})\n",
    "\n",
    "results.append({\n",
    "    'Encoding': 'Agg-state and prefix length 15',\n",
    "    'Model': 'RF regressor',\n",
    "    'Training MAE': prefix_15_train_dt[\"mae\"],\n",
    "    'Test MAE': prefix_15_test_dt[\"mae\"],\n",
    "    'Training MSE': prefix_15_train_dt[\"mse\"],\n",
    "    'Test MSE': prefix_15_test_dt[\"mse\"],\n",
    "    'Training RMSE': prefix_15_train_dt[\"rmse\"],\n",
    "    'Test RMSE': prefix_15_test_dt[\"rmse\"],\n",
    "    'Training R^2': prefix_15_train_dt[\"r2\"],\n",
    "    'Test R^2': prefix_15_test_dt[\"r2\"]\n",
    "})\n",
    "\n",
    "results.append({\n",
    "    'Encoding': 'Last-state',\n",
    "    'Model': 'RF regressor',\n",
    "    'Training MAE': last_state_train_dt[\"mae\"],\n",
    "    'Test MAE': last_state_test_dt[\"mae\"],\n",
    "    'Training MSE': last_state_train_dt[\"mse\"],\n",
    "    'Test MSE': last_state_test_dt[\"mse\"],\n",
    "    'Training RMSE': last_state_train_dt[\"rmse\"],\n",
    "    'Test RMSE': last_state_test_dt[\"rmse\"],\n",
    "    'Training R^2': last_state_train_dt[\"r2\"],\n",
    "    'Test R^2': last_state_test_dt[\"r2\"]\n",
    "})\n",
    "\n",
    "results.append({\n",
    "    'Encoding': 'Last-state + additional features',\n",
    "    'Model': 'RF regressor',\n",
    "    'Training MAE': last_state_additional_train_dt[\"mae\"],\n",
    "    'Test MAE': last_state_additional_test_dt[\"mae\"],\n",
    "    'Training MSE': last_state_additional_train_dt[\"mse\"],\n",
    "    'Test MSE': last_state_additional_test_dt[\"mse\"],\n",
    "    'Training RMSE': last_state_additional_train_dt[\"rmse\"],\n",
    "    'Test RMSE': last_state_additional_test_dt[\"rmse\"],\n",
    "    'Training R^2': last_state_additional_train_dt[\"r2\"],\n",
    "    'Test R^2': last_state_additional_test_dt[\"r2\"]\n",
    "})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b46c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Lalex table\n",
    "print(\"LaTeX Table Format:\\n\")\n",
    "\n",
    "latex_table = results_df.to_latex(\n",
    "    index=False,\n",
    "    float_format=\"%.4f\",\n",
    "    caption=\"Comparison of Model Performance Across Different Encoding Methods\",\n",
    "    label=\"tab:model_comparison\",\n",
    "    position=\"htbp\",\n",
    "    column_format=\"l\" + \"l\" + \"r\"*8\n",
    ")\n",
    "\n",
    "print(latex_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82de0533",
   "metadata": {},
   "source": [
    "## Bonus Tasks \n",
    "\n",
    "We would like to challenge you with the following bonus tasks. For each task that is successfully completed, you may obtain max. 0.5 extra point added to the total 14 points. \n",
    "\n",
    "1. Train a MLP for regression. (If you have used MLP for Task 3, then train another regressor of interest. Evaluate the performance. Explain this in your report.)\n",
    "2. Train an Autoencoder for feature reduction/learning and evaluate whether it helps improve the performance. Explain this in your report.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02745358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apml25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
